{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goal of this notebook is to go through, step-by-step, the entire regression process and related functions/algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outline\n",
    "\n",
    "- Hypothesis function\n",
    "- Cost Function\n",
    "\n",
    "- Gradient descent\n",
    "- Normal Equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothesis Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this function is used to predict an estimation based on the input paramaters x \n",
    "\n",
    "$h_{Θ}(x) = Θ_{0} + Θ_{1}x_{1} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goal of the Cost Function is to choose our various Θ parameters such that h(x) is as close as possible to y for our training examples\n",
    "\n",
    "This is a minimization problem.\n",
    "We need to minimize the sum of squared differences between the h(x) predictions and the y training outcomes.\n",
    "\n",
    "Cost Function Formula:\n",
    "$ J(Θ_{0},Θ_{1}) = 1/2m * \\sum (h_{Θ}x^{(i)}) - y^{(i)})^{2} $\n",
    "\n",
    "\n",
    "where $ h_{Θ}(x^{(i)}) $ is the hypothesis function.\n",
    "\n",
    "\n",
    "The above formula is a squared error cost function, one of the most typically used cost functions for a linear regression model. Minimizing the sum of squared errors gives the most optimal linear fit to the data by minimizing the distance between residuals and the hypothesis predictions. In short, the cost function represents the difference between the predictions and the outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent\n",
    "\n",
    "A method for minimizing the cost function $ J(Θ_{0}, Θ_{1}) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Descent Formula:\n",
    "    \n",
    "$ Θ_{j} := Θ_{j} - \\alpha * \\frac{\\partial}{\\partial Θ_{j}} * J(Θ_{0}, Θ_{1}) $\n",
    "\n",
    "\n",
    "Where $ Θ_{j} $ is updated simultaneously for all values j\n",
    "\n",
    "Alpha represents the learning rate, which is the rate at which the algorithm attempts to converge closer to the minimum of $ J(Θ_{0}, Θ_{1}) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression with Multiple Features\n",
    "\n",
    "When using Linear Regression with Multiple Features, each feature becomes an  $ x_{j} $ and each observation within that feature becomes $ x^{(i)}_{j} $\n",
    "\n",
    "Each feature $ x_{j} $ represents a $ n x 1 $ sized matrix\n",
    "\n",
    "\n",
    "The Linear Regression with Multiple Features equation is the following:\n",
    "\n",
    "$h_{Θ}(x) = Θ_{0} + Θ_{1}x_{1} + Θ_{2}x_{2} + Θ_{3}x_{3} + Θ_{4}x_{4} ... $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For convenience of notation, we can define $ x_{0} = 1 $\n",
    "\n",
    "(This feature represents the y-intercept of the function)\n",
    "\n",
    "which allows us to convert the Hypothesis Function to the following:\n",
    "\n",
    "$h_{Θ}(x) = Θ_{0}x_{0} + Θ_{1}x_{1} + Θ_{2}x_{2} + Θ_{3}x_{3} + Θ_{4}x_{4} ... $  where $ Θ_{0}x_{0} = 1 $ \n",
    "\n",
    "The reason we like to convert the function to look like the above is because we can then simplify it even further by taking advantage of Linear Algebra matrix multiplication. Since $ Θ $ and $ x $ are both $ n x 1 $ matrices, we can transpose one of them to facilitate the matrix multiplication. \n",
    "\n",
    "$ Θ^{T} = [Θ_{0} Θ_{1} Θ_{2} Θ_{3} ... Θ_{n}] $\n",
    "\n",
    "and x is a $ n x 1 $ dimension matrix still (without transposing this wouldn't work because you can't multiply 2 $ n x 1 $ matrices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the hypothesis function can be quickly summarized by the following formula:\n",
    "    \n",
    "$h_{Θ}(x) = Θ^{T}x  $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent for Multiple Variables\n",
    "\n",
    "A method for minimizing the cost function $ J(Θ_{0}, Θ_{1},..., Θ_{n}) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By simplifying the Hypothesis Function to $ h_{Θ}(x) = Θ^{T}x  $, we can also simplify the parameters $  Θ_{0}, Θ_{1},..., Θ_{n} $ to simply $ Θ $ (which is a m x n sized matrix (where n = number of features and n = number of observations) \n",
    "\n",
    "So the Cost Function can be represented as:\n",
    "\n",
    "$ J(Θ_{0},Θ_{1},...,Θ_{n}) = 1/2m * \\sum (h_{Θ}x^{(i)}) - y^{(i)})^{2} $\n",
    "\n",
    "or\n",
    "\n",
    "$ J(Θ) = 1/2m * \\sum ({Θ}^{T}x^{(i)}) - y^{(i)})^{2} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Gradient Descent algorithm with multiple variables then becomes:\n",
    "    \n",
    "Repeat {\n",
    "    \n",
    "$ Θ_{j} := Θ_{j} - \\alpha * \\frac{\\partial}{\\partial Θ_{j}} * J(Θ) $\n",
    "\n",
    "\n",
    "Where $ Θ_{j} $ is updated simultaneously for all values j\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate $ \\alpha $ \n",
    "\n",
    "During Gradient descent, the Cost Function output is being minimized throughout several iterations (sometimes 100s or 1000s or millions). To test that Gradient Descent is working properly, the Cost Function can be plotted as a function of the number of iterations and should show a consistent negative slope. If the Cost Function is decreasing with every iteration, we know it is properly minimizing the Cost Function (i.e. the model error). When the slope of the curve (i.e. the derivative) is zero, then the Gradient Descent has converged.\n",
    "\n",
    "**Plotting the Cost Function over the number of descent iterations is a great way to visualize and confirm that the algorithm is working properly**\n",
    "\n",
    "What if the Gradient Descent is not working correctly (i.e. slope begins to increase at any point)??\n",
    "\n",
    "This is a signal that the Learning Rate $ \\alpha $ is too large. The Learning Rate is the magnitude in step at which the algorithm attempts to converge to the minima. If the Learning Rate is too large, it is possible for the algorithm to overshoot the minima, which can lead to Gradient Descent not working and no convergence. If you make the Learning Rate smll enough (and the algo is working properly), then the Cost Function will decrease at every iteration. The downside to using a small Learning Rate is that it may take long for the algorithm to compute.\n",
    "\n",
    "When determining a Learning Rate to use in a model, you are trying to maximize the Learning Rate while still ensuring the Cost Function is reduced with each iteration. The maximization is simply for computational efficiency and that value will likely need to be trial-and-errored."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Scaling\n",
    "\n",
    "Gradient Descent works effectively if all the features are on a similar scale. If the features vary in scale, the computational time needed to perform the Gradient Descent will increase substantially. By normalizing the features and bringing them to a similar scale, we can make the algorithm's calculations much more efficient. One way to achieve this is by normalizing every feature to a value between -1 and 1. One way of feature scaling is using Mean Normalization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normal Equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

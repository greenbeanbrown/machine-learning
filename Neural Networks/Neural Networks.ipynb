{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outline\n",
    "\n",
    "- Non-Linear Hypotheses\n",
    "- Neurons and the Brain\n",
    "- Model Representation I\n",
    "- Model Representation II\n",
    "- Examples and Intuitions I\n",
    "- Examples and Intuitions II"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-Linear Hypotheses\n",
    "\n",
    "When Neural Networks are used for image recognition, the features become different pixel intensity at certain points in the image. So for example, when recognizing a car, we may create a model with 2 key features, pixel 1 and pixel 2, where pixel 1 is representing a steering wheel and pixel 2 is showing a tire. We can plot the data points of various images on a plot just like any other data problem when we construct it as such. \n",
    "\n",
    "See below for example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![CAR_NEURAL_NETWORK_EXAMPLE](../img/car_neural_network_example.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why not use Logistic Regression to classify the cars then?\n",
    "\n",
    "The problem with using a Logistic Regression model in this case is that your number of features becomes extremely large if you are looking at every pixel as a feature (about 3 million in the above example). Without using so many features it will be very difficult to get a good model fit, but with using Logistic Regression the algorithm is very efficient with so many features (remember we are using Gradient Descent and not Normal Equation with Logistic Regression).\n",
    "\n",
    "So because of this inefficiency, we use a Neural Network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neurons and the Brain\n",
    "\n",
    "Neural Networks were designed to mimic the behavior of the human brain and were previously widely used in the 80s and early 90s, but have fallen out a bit since. They are making a big resurgence recently due to advanced in modern technologies making them more accessible and practical.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![NEURON](../img/neurons.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The \"One Learning Algorithm\" hypothesis\n",
    "\n",
    "The brain has an interesting behavior that allows different parts of the brain to learn to execute different behaviors, i.e. auditory cortex learning to see when re-connected to eyes. This idea is translatable to Machine Learning because the theory behind Neural Networks tries to mimic this in a way by learning to do various behaviors with a single model, i.e. letting the model learn itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Representation I\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neurons in the brain accept many different signals as inputs and then deliver an output through its axon. The Neural Network is designed using a very similar model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![NEURON_MODEL](../img/neuron_model.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above diagram represents a single 'neuron' in a Neural Network. The below diagram represents how they play into each other to make up a network in a Neural Network model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![NEURAL_NETWORK_MODEL](../img/neural_network_model.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 'hidden layer' is called as such because we only can see or know the parameters $x$ and $y$ - the $a^{(2)}$ values are unknown outside of the model, like a black box. \n",
    "\n",
    "The below image explains the mathematical representation of the model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![NEURAL_NETWORK_MATH](../img/neural_network_math.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key takeaways here are that the superscript above the variables is representing the layer in the network. So for example, $Î˜^{(1)} $ represents the theta parameter in layer 1 (which is an input to layer 2). The variable $a^{(2)}_{i}$ represents the hidden units in layer 2, which are inputs to layer 3. In the above representation, the variable $a^{(3)}_{1}$ represents the 3rd and final layer, which is also the hypothesis function. It is important to note that different Neural Networks can have more layers and be much more complex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Representation II\n",
    "\n",
    "The Neural Network model can also be implemented in vectorized form using something called Forward Propagation.\n",
    "\n",
    "**Forward Propagation:** Start off with the activations of input units and propagate that to the hidden layer, include the activations of the hidden layer, then propagate that to get the activations of the output layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![FORWARD_PROPAGATION](../img/forward_propagation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you ignore the 1st layer (input layer) of the Neural Network model, it basically becomes the Logistic Regression model - where you have the model features in Layer 2 (hidden layer) which are used in the Layer 3 as inputs to the hypothesis function. \n",
    "\n",
    "In the Neural Network model, the Layer 2 values are actually learned as functions from the Layer 1 inputs. So the Neural Network is NOT constrained by just the inputs $x_{1}, x_{2}, x_{3}$ because the 2nd Layer will transform those features into different features, which are then finally passed on to Layer 3. So essentially the Neural Network is kind of like a chain of Logistic Regressions, where each input layer is building off the previous layer to create better feature parameters for the next layer. \n",
    "\n",
    "The algorithm has the flexibility to learn whatever features it wants in order to create the most optimal hypothesis output in the end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![NEURAL_NETWORK_LEARNING](../img/neural_network_learning.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![COMPLEX_NEURAL_NETWORK](../img/complex_neural_network.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples and Intuitions I\n",
    "\n",
    "In simple non-linear classification, the functions can be described with XOR, XNOR (NOT OR), AND, and OR logic. That is, the functions end up representing the logic of these operators because of how the sigmoid function behaves at it's asymptotes.\n",
    "\n",
    "The below example demonstrates this using a function mimicking AND logic:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![NEURAL_NETWORK_EXAMPLE](../img/neural_network_example.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples and Intuitions II\n",
    "\n",
    "In addition to the previous logic operators, there is also negation, or NOT logic ($x_{1} = 0$ and $x_{2} = 0$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ![NEURAL_NETWORK_NEGATION](../img/neural_network_negation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a more complex network that leverages multiple types of logics can create a more robust model for non-linear classifications:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ![NEURAL_NETWORK_MULTIPLE_LOGIC](../img/neural_network_multiple_logic.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Class Classification\n",
    "\n",
    "By extending the one-vs-all approach that we discussed earlier in multi-class classification using Logistic Regression, we can perform the same using a Neural Network. \n",
    "\n",
    "Each classification is represented by different variations of hypothesis output vectors. \n",
    "\n",
    "In the example below, each classification has a corresponding output vector that represents it:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![NEURAL_NETWORK_MULTI_CLASS](../img/neural_network_multi_class.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Regression\n",
    "\n",
    "Also called L2 Regularization\n",
    "\n",
    "Related Topics:\n",
    "- Regularization\n",
    "- Lasso Regression\n",
    "- Linear and Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ridge Regression is a regularization technique that can be applied to models to avoid overfitting training data. This can result in better predictions in the long run, as the model variance is reduced and the model can be generalized well.\n",
    "\n",
    "In short, we are shrinking model parameters in order to decrease Variance and increase Bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![RIDGE_REGRESSION_CONCEPT](../img/ridge_regression_concept.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whereas in Linear Regression (ordinary least squares), the model coefficients are determined by minimizing the **sum of squared residuals**, the gradient descent of Ridge Regression aims to minimize **sum of squared residuals + regularization parameter**. \n",
    "\n",
    "The regularization parameter is $\\lambda$ x slope$^{2}$ and allows for more bias in the model. Squaring the slope adds a penalty to the ordinary least squares method and Lambda determines the magnitude of that penalty.\n",
    "\n",
    "Higher slope = higher penalty\n",
    "\n",
    "\n",
    "Higher lambda = higher penalty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![RIDGE_REGRESSION_DIFFERENCE](../img/ridge_regression_difference.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ultimate goal of Ridge Regression is to introduce a small amount of Bias in order to disproportionately decrease the long term Variance of the model. Therefore, Ridge Regression should be used when this trade off makes economic sense - in other words, you should only use Ridge Regression if a small increase in bias can generate a large enough decrease in variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does lambda work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lambda can be any positive value from 0 to infinity.\n",
    "\n",
    "If Lambda is 0, then the model performs identically to a normal Linear Regression (it's cancelling out the entire regularization term).\n",
    "\n",
    "The slope will get asymptotic as Lambda approaches infinity, i.e. the higher the Lambda value, the flatter the slope of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How is the Lambda value determined?\n",
    "\n",
    "You typically try a bunch of sample values for Lambda and then use Cross-Validation (k-fold Cross-Validation) to determine which value results in the lowest model variance. In other words, trial-and-error through cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Regression can also be applied to Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![RIDGE_REGRESSION_CONCEPT](../img/ridge_regression_concept.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Regression without a large dataset or sample size\n",
    "\n",
    "Ridge Regression is super useful in cases where the Linear Model has lots of parameters and relatively few data observations. \n",
    "\n",
    "For example, Ridge Regression is a fantastic technique to use in the case of gene modeling, where there could be 10,000 or more features based on mice gene expression measurements. Because collecting this data can be very expensive, we may only have 500 observations (rather than, ideally a much larger number like 10,000). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![RIDGE_REGRESSION_USE_CASE](../img/ridge_regression_use_case.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

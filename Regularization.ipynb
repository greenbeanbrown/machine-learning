{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization\n",
    "\n",
    "Machine learning models need to generalize well to new examples that the model has not seen in practice. If the model is overfitting, we can use Regularization techniques to improve the model prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outline\n",
    "\n",
    "- The Problem of Overfitting\n",
    "- Cost Function\n",
    "- Regularized Linear Regression\n",
    "- Regularized Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Problem of Overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![LINEAR_OVERFITTING](./img/linear_overfitting.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![LOGISTIC_OVERFITTING](./img/logistic_overfitting.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Underfitting\n",
    "\"Underfitting\" is another way of saying a model has high bias. This can happen when a model doesn't have enough features.\n",
    "\n",
    "#### Addressing Underfitting\n",
    "\n",
    "#### Overfitting\n",
    "\"Overfitting\" is another way of saying a model has high variance. This can happen when a model has a lot of features or when there is not a sufficient number of observations in the training data.\n",
    "\n",
    "#### Addressing Overfitting\n",
    "- Reduce number of features\n",
    "    - Can manually do this or use an algorithm to automate it\n",
    "    \n",
    "- Regularization\n",
    "    - Keep the features, but reduce the magnitude of $ θ $ parameters\n",
    "        - This works well when we need to keep a lot of features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost Function \n",
    "\n",
    "Regularization involves punishing certain $θ$ parameters and making them smaller (basically 0) to get to a simpler hypothesis that is less prone to overfitting.\n",
    "\n",
    "To do this, you add a regularization term to the Cost Function:\n",
    "\n",
    "Regularization Term = $\\lambda \\sum θ^{2}_{j}$\n",
    "\n",
    "Cost Function with Regularization Term:\n",
    "\n",
    "$ J(θ) = 1/2m * \\sum (h_{θ}x^{(i)}) - y^{(i)})^{2} + \\lambda \\sum θ^{2}_{j}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![REGULARIZATION](./img/regularization_example.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What does the regularization parameter do?\n",
    "\n",
    "The parameter lambda is controlling the trade off between fitting the data well and also keeping the θ parameters small. This results in a more smooth, less overfit curve when plotted.\n",
    "\n",
    "If lambda is set to a very large value, it can result in the model actually underfitting the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularized Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularized Logistic Regression"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

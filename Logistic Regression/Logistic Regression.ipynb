{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression is a model used in classification problems where the desired prediction is a discrete value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Classification Problems\n",
    "\n",
    "- Identifying email spam\n",
    "- Determining if online ecommerce transactions are fraudulent\n",
    "- Classifying tumors as malignant or benign"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outline\n",
    "\n",
    "- Linear vs Logistic Regression for Classification\n",
    "- Hypothesis Function\n",
    "- Interpretation of Hypothesis Output\n",
    "- Decision Boundary\n",
    "--------\n",
    "- Cost Function\n",
    "--------\n",
    "- Simplified Cost Function\n",
    "- Gradient Descent\n",
    "- Advanced Optimization\n",
    "- Multiclass Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear vs Logistic Regression for Classification\n",
    "\n",
    "Applying Linear Regression to a classification problem is typically not a good idea because of the discrete nature of the y values.\n",
    "\n",
    "- Outliers can heavily skew the model and dramatically hinder the accuracy of the model\n",
    "- Another reason is that in Linear Regression the output of the hypothesis function can be > 1 or < 0 despite the classification values only lying within the range of 0 or 1\n",
    "\n",
    "**Because of these limitations, we ditch the linear model and adopt the logistic model for classifcation problems**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothesis Function\n",
    "\n",
    "Since we want our output values to be contained within 0 and 1, we want to use a Sigmoid or Logistic function as the basis for our hypothesis function.\n",
    "\n",
    "In order to step from the Linear model to the Logistic model, we start with our same Linear hypothesis function:\n",
    "\n",
    "$h_{Θ}(x) = Θ^{T}x  $\n",
    "\n",
    "In order to ensure the output values are between 0 and 1, we pass the hypothesis function as a parameter to a Sigmoid/Logistic function $ g(z) $:\n",
    "\n",
    "$g(z) = \\frac{1}{1 + e^{-z}} $\n",
    "\n",
    "Which means the hypothesis function ultimately becomes equivalent to:\n",
    "\n",
    "$h_{Θ}(x) = \\frac{1}{1 + e^{-Θ^{T}x}} $\n",
    "\n",
    "This function is a representation of the Sigmoid/Logistic function which assimptotes at 0 for values approaching -inf and assimptotes at 1 for values approaching inf. All of the function's return values are within the range of 0 and 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation of Hypthothesis Output\n",
    "\n",
    "The hypothesis function output contains a series of probabilities representing the likelihood of y = 1 (i.e. a positive classification).\n",
    "\n",
    "For example, in a problem where we are trying to classify malignant tumors (y=1), then if $h_{Θ}(x) = 0.7 $, we can say there is a 70% chance that the tumor is malignant based on the feature values. Conversely, the probability that the tumor is benign (y=0) would be 30% in this example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Boundary\n",
    "\n",
    "The Decision Boundary is a system of inequalities that define the points where $y = 1$.\n",
    "\n",
    "**A very simple example:**\n",
    "\n",
    "Suppose we predict $y=1$ if $h_{θ}(x) \\ge 0.5$\n",
    "\n",
    "and we predict $y=0$ if $h_{θ}(x) \\lt 0.5$\n",
    "\n",
    "**A harder example:**\n",
    "\n",
    "Suppose our hypothesis function looks like this:\n",
    "\n",
    "$h_{θ}(x) = g(θ_{0} + θ_{1}x_{1} + θ_{2}x_{2})$\n",
    "\n",
    "and for whatever reason we know that the θ parameters are as follows:\n",
    "\n",
    "$θ_{0}=-3, θ_{1}=1, θ_{2}=1$\n",
    "\n",
    "Which can also be represented in matrix form like this:\n",
    "\n",
    "θ = $\\begin{bmatrix}\n",
    "-3\\\\\n",
    "1\\\\\n",
    "1\\\\\n",
    "\\end{bmatrix}$\n",
    "\n",
    "Our decision boundary then would look like this:\n",
    "\n",
    "Predict $y=1$ if $-3 + x_{1} + x_{2} \\ge 0$\n",
    "\n",
    "**Note that $-3 + x_{1} + x_{2}$ is just $θ^Tx$**\n",
    "\n",
    "The decision boundary can be simplified to this:\n",
    "\n",
    "$x_{1} + x_{2} \\ge 3$\n",
    "\n",
    "You can visualize this equation pretty easily by plugging in values for $x_{1}$ and $x_{2}$. Start with the x and y-intercepts, especially for a simple linear inequality like this example.\n",
    "\n",
    "The points along the actual Decision Boundary represent $h_{θ}(x) = 0.5$, or in other words, a perfect split in probability of $y=1$ or $y=0$. This is essentially the inflection point and the probability will increase or decrease as you get farther from these points in a certain direction. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![DECISION_BOUNDARY](../img/decision_boundary.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-Linear Decision Boundaries\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![MULTI_DECISION_BOUNDARY](../img/multi_decision_boundary.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### NOTE ON DECISION BOUNDARIES\n",
    "\n",
    "The Decision Boundary is a function of the Hypothesis function and NOT the dataset characteristics. The Hypothesis function is derived from the dataset and is used to determine the theta parameters, but the actual dataset does not determine the Decision Boundary because its dependent on the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost Function\n",
    "\n",
    "REMINDER: Fitting the Cost Function is really just solving for the theta parameters and fitting them to the dataset - it's that simple in concept.\n",
    "\n",
    "##### Can we use the same Cost Function as in Linear Regression?\n",
    "\n",
    "No, because the function would be non-convex in a Logistic Regression model because of the non-linearity of the Sigmoid function (it naturally creates a non-convex function). This is important because a non-convex function would result in many local minima, making it unreliable to use Gradient Descent in an attempt to find the global minimum. Remember, the goal of Gradient Descent and what the Cost Function is ultimately used for, is minimization! A convex function ensures that the Gradient Descent algorithm will find the global minimum because it is identical to the local minimum.\n",
    "\n",
    "See below for visual representation of non-convex and convex functions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![CONVEX_COST_FUNCTIONS](../img/convex_cost_functions.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Cost Function\n",
    "\n",
    "For the reasons mentioned above, we use a different Cost Function in Logistic Regression as opposed to Linear Regression (to avoid non-convex functions).\n",
    "\n",
    "We need to create a Cost Function that is equal to 0 if the prediction is correct (i.e.  $h_{θ}(x)=1, y=1$), but also approaches infinity as $h_{θ}(x)$ approaches zero. The idea here is we need to heavily penalize the algorithm based on how far off the Hypothesis Function output is from the actual observation.\n",
    "\n",
    "The new Cost Function we will use for Logistic Regression is the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![LOGISTIC_COST_FUNCTION](../img/logistic_cost_function.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

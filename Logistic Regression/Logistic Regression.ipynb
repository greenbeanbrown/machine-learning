{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression is a model used in classification problems where the desired prediction is a discrete value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Classification Problems\n",
    "\n",
    "- Identifying email spam\n",
    "- Determining if online ecommerce transactions are fraudulent\n",
    "- Classifying tumors as malignant or benign"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outline\n",
    "\n",
    "- Linear vs Logistic Regression for Classification\n",
    "- Hypothesis Function\n",
    "- Interpretation of Hypothesis Output\n",
    "- Decision Boundary\n",
    "- Cost Function\n",
    "- Simplified Cost Function\n",
    "- Gradient Descent\n",
    "- Advanced Optimization\n",
    "- Multi-Class Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear vs Logistic Regression for Classification\n",
    "\n",
    "Applying Linear Regression to a classification problem is typically not a good idea because of the discrete nature of the y values.\n",
    "\n",
    "- Outliers can heavily skew the model and dramatically hinder the accuracy of the model\n",
    "- Another reason is that in Linear Regression the output of the hypothesis function can be > 1 or < 0 despite the classification values only lying within the range of 0 or 1\n",
    "\n",
    "**Because of these limitations, we ditch the linear model and adopt the logistic model for classifcation problems**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothesis Function\n",
    "\n",
    "Since we want our output values to be contained within 0 and 1, we want to use a Sigmoid or Logistic function as the basis for our hypothesis function.\n",
    "\n",
    "In order to step from the Linear model to the Logistic model, we start with our same Linear hypothesis function:\n",
    "\n",
    "$h_{Θ}(x) = Θ^{T}x  $\n",
    "\n",
    "In order to ensure the output values are between 0 and 1, we pass the hypothesis function as a parameter to a Sigmoid/Logistic function $ g(z) $:\n",
    "\n",
    "$g(z) = \\frac{1}{1 + e^{-z}} $\n",
    "\n",
    "Which means the hypothesis function ultimately becomes equivalent to:\n",
    "\n",
    "$h_{Θ}(x) = \\frac{1}{1 + e^{-Θ^{T}x}} $\n",
    "\n",
    "This function is a representation of the Sigmoid/Logistic function which assimptotes at 0 for values approaching -inf and assimptotes at 1 for values approaching inf. All of the function's return values are within the range of 0 and 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation of Hypthothesis Output\n",
    "\n",
    "The hypothesis function output contains a series of probabilities representing the likelihood of y = 1 (i.e. a positive classification).\n",
    "\n",
    "For example, in a problem where we are trying to classify malignant tumors (y=1), then if $h_{Θ}(x) = 0.7 $, we can say there is a 70% chance that the tumor is malignant based on the feature values. Conversely, the probability that the tumor is benign (y=0) would be 30% in this example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Boundary\n",
    "\n",
    "The Decision Boundary is a system of inequalities that define the points where $y = 1$.\n",
    "\n",
    "**A very simple example:**\n",
    "\n",
    "Suppose we predict $y=1$ if $h_{θ}(x) \\ge 0.5$\n",
    "\n",
    "and we predict $y=0$ if $h_{θ}(x) \\lt 0.5$\n",
    "\n",
    "**A harder example:**\n",
    "\n",
    "Suppose our hypothesis function looks like this:\n",
    "\n",
    "$h_{θ}(x) = g(θ_{0} + θ_{1}x_{1} + θ_{2}x_{2})$\n",
    "\n",
    "and for whatever reason we know that the θ parameters are as follows:\n",
    "\n",
    "$θ_{0}=-3, θ_{1}=1, θ_{2}=1$\n",
    "\n",
    "Which can also be represented in matrix form like this:\n",
    "\n",
    "θ = $\\begin{bmatrix}\n",
    "-3\\\\\n",
    "1\\\\\n",
    "1\\\\\n",
    "\\end{bmatrix}$\n",
    "\n",
    "Our decision boundary then would look like this:\n",
    "\n",
    "Predict $y=1$ if $-3 + x_{1} + x_{2} \\ge 0$\n",
    "\n",
    "**Note that $-3 + x_{1} + x_{2}$ is just $θ^Tx$**\n",
    "\n",
    "The decision boundary can be simplified to this:\n",
    "\n",
    "$x_{1} + x_{2} \\ge 3$\n",
    "\n",
    "You can visualize this equation pretty easily by plugging in values for $x_{1}$ and $x_{2}$. Start with the x and y-intercepts, especially for a simple linear inequality like this example.\n",
    "\n",
    "The points along the actual Decision Boundary represent $h_{θ}(x) = 0.5$, or in other words, a perfect split in probability of $y=1$ or $y=0$. This is essentially the inflection point and the probability will increase or decrease as you get farther from these points in a certain direction. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![DECISION_BOUNDARY](../img/decision_boundary.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-Linear Decision Boundaries\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![MULTI_DECISION_BOUNDARY](../img/multi_decision_boundary.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### NOTE ON DECISION BOUNDARIES\n",
    "\n",
    "The Decision Boundary is a function of the Hypothesis function and NOT the dataset characteristics. The Hypothesis function is derived from the dataset and is used to determine the theta parameters, but the actual dataset does not determine the Decision Boundary because its dependent on the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost Function\n",
    "\n",
    "REMINDER: Fitting the Cost Function is really just solving for the theta parameters and fitting them to the dataset - it's that simple in concept.\n",
    "\n",
    "##### Can we use the same Cost Function as in Linear Regression?\n",
    "\n",
    "No, because the function would be non-convex in a Logistic Regression model because of the non-linearity of the Sigmoid function (it naturally creates a non-convex function). This is important because a non-convex function would result in many local minima, making it unreliable to use Gradient Descent in an attempt to find the global minimum. Remember, the goal of Gradient Descent and what the Cost Function is ultimately used for, is minimization! A convex function ensures that the Gradient Descent algorithm will find the global minimum because it is identical to the local minimum.\n",
    "\n",
    "See below for visual representation of non-convex and convex functions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![CONVEX_COST_FUNCTIONS](../img/convex_cost_functions.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Cost Function\n",
    "\n",
    "For the reasons mentioned above, we use a different Cost Function in Logistic Regression as opposed to Linear Regression (to avoid non-convex functions).\n",
    "\n",
    "We need to create a Cost Function that is equal to 0 if the prediction is correct (i.e.  $h_{θ}(x)=1, y=1$), but also approaches infinity as $h_{θ}(x)$ approaches zero. The idea here is we need to heavily penalize the algorithm based on how far off the Hypothesis Function output is from the actual observation.\n",
    "\n",
    "The new Cost Function we will use for Logistic Regression is the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![LOGISTIC_COST_FUNCTION](../img/logistic_cost_function.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simplified Cost Function\n",
    "\n",
    "Due to the binary output of the Logistic Hypothesis Function, we can greatly simplify the system of equations shown above. The equation is below and is possible because the 0 or 1 will cancel out terms that make the simplified equation equivalent to the original Cost Function.\n",
    "\n",
    "If we simplify the above Cost Function to the following for demonstration:\n",
    "\n",
    "$J(θ) = \\frac{1}{m} \\sum Cost(h_{θ}(x^{(i)}), y^{(i)}) $\n",
    "\n",
    "Then we can define $Cost(h_{θ}(x^{(i)}), y^{(i)}) $ as:\n",
    "\n",
    "$Cost(h_{θ}(x)^{(i)}) = -ylog(h_{θ}(x)) - (1-y)log(1-h_{θ}(x))$\n",
    "\n",
    "And derive our final Cost Function:\n",
    "\n",
    "$J(θ) =  -\\frac{1}{m}[ \\sum y^{(i)}log(h_{θ}(x^{(i)})) + (1-y^{(i)})log(1-h_{θ}(x^{(i)}))]$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we have our Hypothesis and Cost Functions for Logistic Regression. The next step is to fit the $θ$ parameters, which we do by minimizing the Cost Function. Once we know our optimal $θ$ values, we plug them into the Hypothesis Function and predict the probability that an observation will be $y=1$ or $y=0$ for any given $x$ feature observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent\n",
    "\n",
    "The Gradient Descent algorithm in Logistic Regression is identical to the Linear Regression version, with the only exception being that the $h_{θ}(x)$ is the Logistic function. You simply need to use the correct Hypothesis Function for the Linear or Logistic Regression and the algorithm will operate identically from a mathematics perspective.\n",
    "\n",
    "Gradient Descent Formula:\n",
    "\n",
    "Repeat \\{\n",
    "\n",
    "$Θ_{j} := Θ_{j} - \\alpha * \\frac{\\partial}{\\partial Θ_{j}} * J(Θ_{0}, Θ_{1}) $\n",
    "\n",
    "}\n",
    "\n",
    "Where $ Θ_{j} $ is updated simultaneously for all values j\n",
    "\n",
    "Alpha represents the learning rate, which is the rate at which the algorithm attempts to converge closer to the minimum of $ J(Θ_{0}, Θ_{1}) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced Optimization\n",
    "\n",
    "In addition to Gradient Descent, there are other optimization algorithms available to minimize the Cost Function. These other methods provide advantages like not need to manually select an alpha and being faster. These algorithms should not be used without fully understanding the underlying methodology as their flexibility provides more room for user error.\n",
    "\n",
    "Examples of some of these other algorithms:\n",
    "-Conjugate Gradient\n",
    "-BFGS\n",
    "-L-BFGS\n",
    "\n",
    "These all have different advantages and disadvantages, but their complexity is beyond the scope of this notebook and beginner Machine Learning course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Class Classification\n",
    "\n",
    "While we have previously looked at Logistic Regression to classify a single binary outcome, the model can also be used to make many different binary classifications in a dataset.\n",
    "\n",
    "For example:\n",
    "\n",
    "Email tagging: Work, Friends, Family\n",
    "Medical Diagrams: Not Ill, Flu, Cold\n",
    "Weather: Sunny, Cloudy, Rainy\n",
    "\n",
    "Using a One-vs-All classification method allows us to make these different classifications within a single model. This method works by isolating each different classifation and assigning that class as the positive class (i.e. $y=1$) and assign ALL other classes as the negative class (i.e. $y=0$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ONE_VS_ALL](../img/one_vs_all_classification.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
